% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoMLModel.R
\name{autoMLmodel}
\alias{autoMLmodel}
\title{Automated machine learning training of models}
\usage{
autoMLmodel(train, test = NULL, target = NULL, maxLevels = 100,
  testSplit = 0.2, tuneIters = 200, tuneType = "random",
  gridItune = 15, models = "all", varImp = 20, liftGroup = 50,
  maxObs = 40000, uid = NULL, seed = 1991)
}
\arguments{
\item{train}{[data.frame | Required] Training set}

\item{test}{[data.frame | Optional] Optional testing set to validate models on. If none is provided, one will be created internally. Default of NULL}

\item{target}{[integer | Required] If a target is provided classification or regression models will be trained, if left as NULL unsupervised models will be trained. Default of NULL}

\item{maxLevels}{[integer | Optional] Number of unique values in target feature before the problem type is seen as a regression problem. Default of 100}

\item{testSplit}{[numeric | Optional] Percentage of data to allocate to the test set. Stratified sampling is done. Default of 0.1}

\item{tuneIters}{[integer | Optional] Number of tuning iterations to search for optimal hyper parameters. Default of 10}

\item{tuneType}{[character | Optional] Tune method applied, options are: random, frace and grid. random uses random tuning and frace uses iterated f-racing algorithm for the best solution. Default of random}

\item{gridItune}{[integer | Optional] Number of tuning iterations to grid search for optimal hyper parameters. Default of 15, this is only applicable if tuneType is grid}

\item{models}{[character | Optional] Which models to train. Default of all. Available models are logistic regression, glment, rpart, xgboost, ranger and randomForest}

\item{varImp}{[integer | Optional] Number of important features to plot}

\item{liftGroup}{[integer | Optional] NUmber of lift value to validate the test model performance}

\item{maxObs}{[numeric | Optional] Number of observations in the experiment training set on which models are trained, tuned and resampled on. Default of 40000. If the training set has less than 40k observations all will be used}

\item{uid}{[character | Optional] unique variable to keep in test output data}

\item{seed}{[integer | Optional] Random number seed for reproducible results}
}
\value{
List output contains trained models and results
}
\description{
Automated training, tuning and validation of machine learning models. Models are  tuned and resampling validated on an experiment set and trained on the full set and validated and testing on external sets. Classification models tune the probability threshold automatically and returns the results. Each model contains information of performance, the trained model as well as some plots.
}
\details{
all the models trained using mlr train function, all of the functionality in mlr package can be applied to the autoMLmodel outcome

autoMLmodel provides below information of the machine learning classification models
\describe{
  \item{\code{trainedModels}}{Model level list output contains trained model object, hyper parameters, tuned data, test data, performance and Model plots}
  \item{\code{results}}{Summary of all trained model reuslts like AUC, Precision, Recall, F1 score}
  \item{\code{lift}}{Model gain chart}
}
}
\examples{
# Run binary classification model defualt method
mymodel <- autoMLmodel( train = heart, test = NULL, target = 'target_var', testSplit = 0.2,
maxLevels = 100, tuneIters = 10, tuneType = "random",
models = c("xgboost", "ranger"), varImp = 10, liftGroup = 50, maxObs = 4000,
uid = NULL, seed = 1991)

# Run only Logistic regression model
mymodel <- autoMLmodel( train = heart, test = NULL, target = 'target_var', testSplit = 0.2,
maxLevels = 100, tuneIters = 10, tuneType = "random",
models = "logreg", varImp = 10, liftGroup = 50, maxObs = 4000, uid = NULL, seed = 1991)

}
\seealso{
\code{\link[mlr:train]{train}}
\code{\link[mlr:makeLearner]{makeLearner}}
}
